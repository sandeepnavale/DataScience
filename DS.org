* Data Science
** Intro to DS
*** Steps in DS Project pipeline
    1. Planning
       - Define goals.
       - Organize resources.
       - Coordinate people.
       - Schedule the project.
    2. Data Preparation
       - Get Data.
       - Clean data.
       - Explore data.
       - Refine data.
    3. Modeling
       - Create model.
       - Validate model.
       - Evaluate model.
       - Refine model.
    4. Followup
       - Presenting model.
       - Deploy model.
       - Revisit model.
       - Archive assets.
*** Quality of data scientist.
    + Data scientists are able to take unstructured data and find order, meaning & value, which give their clients vital insight & competitive advantages.

*** Choose Metrics
    1. KPI: - non financial, timely, CEO focus, team based, has impact(Profitability, less defects, ),
    2. SMART Goals: Specific, Measurable, Assingable, Realistic, Timebound.
    3. Classification Accuracy: +/-ve events, sensitivity, specificity.
       - Measurement boasts awareness.
       - Awareness contributes to quality.
       - you should measure thoughtfully & sensitively.
*** Data Sources / Existing data sets.
    1. Open data.
       + Govt/Corportate data. enormous data, range, well formated / documentaed.
       + biased sample, meaning of variables are not clear,
    2. In-house data.
       + Quick, easy, free, formatted, team generated data is local.
       + Non existant data, no documentation, quality is not on par.
    3. Third party data.
       + Data as a Service (DaaS)
       + Data brokers.
       + Expensive.
       + Require validation.
    4. API's
       + REST API's, easy, flexible.
    5. Scrapping.
       + Use ready made tools, Ex: import.io, ScrapperWiki, Tabula, GoogleSheets _=IMPORTHTML_, excel.
       + Custom tools: Python, R, Bash, etc.

*** Data Exploration -- sets the stage for modeling.
**** Used for
     + Single distribution
     + Joint Distribution
     + Unsual cases.
     + Errors in Data.
     + Missing Data.
**** Types of chart
     * Bar chart --> catagorical variables
     * Whisker/ Box Plot --> Quantitative variables
     * Histograms --> Quantitative var, Shape of distribution
     * Scatter -->Quantitative var, show associations b/w variables.
**** Numerical Exploration / Statistics.
     Catagories of this.
     1) Robust statistics
     2) Resample Data. -- Cross Validation,Permutation, Bootstrap, Jackknife.
     3) Transform Data -- Tukeys ladder of powers,
*** Decision Trees
    1. Classification Trees.
    2. Regression Trees.
    Algorithms
    + ID3, C5.0, Cart, Chaid, mars,CI,

Pros: flexible, robust,large datasets, whitebox model.
Cons: Heuristics & local optima, overfitting,bias towards more levels

*** Ensembles
    1. Average of estimates.
    2. Diversity helps.

    Methods:
    1) Bagging.
    2) Boosting.
    3) Blending/ Stacking.

*** K-Nearest-Neighbours.
    1. k = num of neighbours
*** Naive Bayes Classifiers
    1. Simple & effective approach
    2. work on variety of predictor.
    3. easy to interpret results.

*** Aritifical Neural Network.
    1. infer complicated rules.
    2. learn from experience.
    3. use a block-box process.
    4.
** Statistics
*** =p-value=
    - Determines how /significant/ the value is regression.
    - value > 0.05 is _LESS SIGNIFICANT_ predictant.
    - value < 0.05 is _MORE SIGNIFICANT_ predictant.
*** =r-Value= <<<rvalue>>>
*** =mean= : center of the data. Weighted mean.
    :LOGBOOK:
    CLOCK: [2018-06-15 Fri 14:29]--[2018-06-15 Fri 14:56] =>  0:27
    CLOCK: [2018-06-14 Thu 19:16]--[2018-06-14 Thu 19:41] =>  0:25
    :END:
*** =meidan= : mid point
*** =Standard Deviation=:
*** Data Distribution 
**** Continious Distribution
**** Uniform Distribution
**** Normal Distribution
**** Poisson Distribution
**** Binomial Distribution
*** Statistical Experiments & Significance tesing. 
    - Design of experiements is essential & continous activity of Data scientist.
**** A/B Testing <<A/B TESTING>>
     + Two groups are compared to know who is superior.
     + =Treatment= : something (drug, price, webheadline) to which
       subject is exposed.
     + =Treatment Group=: A group of subjects exposed to specific treatment.
     + =Control Group=: A group of subjects which are NOT exposed to any
       treatment.
     + =Randomization=: The process of randomly assigning subjects to treatment.
     + =Subjects=:The items (patients, web pages, etc) which are exposed
       to treatment.
     + =test statistic=: The metric used to measure the treatment.
**** Hypothisis/Significance Testing
     Purpose is to help you learn whether random chance might be
     responsible for an observed effect.
     =Null hypothesis=: The hypothisis that chance is to blame.
     =Alternative Hypothesis=: Counterpoint to null (what you hope to prove)
     =one-way test=: test which count "chance" results in one direction.
     =two-way test=: test which counts "chance" results in two direction.
     
     + 
**** Resampling
**** Significance & P-values
**** t Tests
**** Multiple testing
**** Degree of Freedom
**** ANOVA
**** Chi-Square Test
**** MultiArm Bandit Algorithms
     when we have only 2 groups use [[A/B TESTING]], when more than 2,
     then use this method.

**** Power & Sample size.
** TODO Data Mining.
*** Find Patterns in data.
**** Simplify  
     + Reduce noise, 
     + Reduce dimentionality, 
     + Find important variables or combinations.
**** find groups.
    + Detect Clusters
    + Detect Classification.
    + Association Analysis.
    + Anomaly detection.
**** Predict scores.
     + Regression models.
**** Data Mining tasks.
***** Sequence Mining.
      Time-Series analysis.
***** Text Mining.
      Analyse text files.
***** Apporaches.
      + Classical Statistics.
      + Machine learning. [file:ML.org]

***** DONE CRISP-DM & 9 Laws (Data Mining methodology)
      CLOSED: [2018-08-10 Fri 12:53]
      :LOGBOOK:
      - State "DONE"       from "NEXT"       [2018-08-10 Fri 12:53]
      :END:
      + CRISP-DM
        Cross Industry Standard Process for Data Mining (CRISP-DM)
        6 Major phases.
        1. Busingess understanding.
        2. Data Understanding.
        3. Data prepration.
        4. Modeling.
        5. Evaluation.
        6. Deployment

      + /9-Laws of Data mining./
        1. Business OBJECTIVES are origing of data mining solution.
        2. Business KNOWLEDGE is central to every step of data mining process.
        3. Data preparation is more than 50% of data mining work.
        4. No free lunch for data minner. right model is only discovered by experiment.
           study problem apply ensemble, neural network, etc.
        5. There are always the pattern. have business knowledge to know patterns.
        6. DM amplifies the perception in business domain.
        7. Prediction increases information locally by generalization.
        8. Value law: the value of result is not determined by accuracy or stability of predictive.
        9. Law of change: all patterns are subject to change.

*** Algorithms for Data reduction.
    1. Linear methods
       Principal Components Analysis(PCA) -- Reduces number of data. is the most commonly used.
    2. NonLinear Methods --> high dimenstional data.
       + Kernel PCA
       + Isomap
       + Locally linear embedding.
       + Maximum variance unfolding.

*** Classifications
**** Unsupervised
**** Supervised

*Anomoly Vs Outliers*
Anomoly: is the indicator of problem. (unexpected/unsual symptom)
Outliers: is used to detect anamoly, its a general catagory. 


** DONE NEXT Pandas [33%]

   1. [X] User df.xs() to explore the MultiIndex DF's
   2. [ ] Pandas Panel
   3. [ ] Method Chainging
   4. 

** Matplotlib
   1. Functional mode of drawing.
   2. Object Oriented way of plotting.
** Mastering concepts.
*** EcoSystem Overview
#+ATTR_HTML: :width 50% :height 50%
    file:figs/DS_Overview.png

    /Data Hub/
#+ATTR_HTML: :width 50% :height 50%
    [[file:figs/DS_DataHub.png]]

    /Star Schema/
#+ATTR_HTML: :width 50% :height 50%
    [[file:figs/DS_StartSchema.png]]
#+ATTR_HTML: :width 50% :height 50%
    [[file:figs/DS_StarSchema2.png]]

*** Staging Data
    1. Loading & profiling data
    2. Data quality testing (null checks, duplicates, etc.)
*** Cleansing Data
    1. Adding data types. (conversion of datatypes, astype(), etc.)
    2. Handling missing values. (drop na)
*** Conforming Data.
